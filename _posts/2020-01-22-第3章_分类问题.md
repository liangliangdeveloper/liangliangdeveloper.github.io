---
layout: article
title: 第3章 分类问题
mathjax: true
articles:
  excerpt_type: html
tags: [深度学习, TensorFlow深度学习]
key: c3tensorflow
comment: true
---

## 3.1 手写数字图片数据集

为了方便业界统一测试和评估算法，发布了手写数字图片数据集，命名为MNIST，包含0-9共10个数字的手写照片。

每张照片均被缩放为28×28的大小，并且只保留灰度信息。

灰度图现在表示为0-255,0是黑色，255是白色。形状是$[h,w]$或者$[h,w,1]$。RGB的图片用3个通道表示，形状为$[h,w,3]$.

TensorFlow可以自动下载MNIST数据集。

```python
(x, y),(x_val, y_val) = datasets.mnist.load_data()
x = 2 * tf.convert_to_tensor(x, dtype = tf.float32)/255.-1
y = tf.convert_to_tensor(y, dtype = tf.int32)
y = tf.one_hot(y, depth=10)
print(x.shape, y.shape)
train_dataset = tf.data.Dataset.from_tensor_slices((x, y))
train_dataset = train_dataset.batch(512)
```

`load_data()`返回两个元组对象，一个是训练集，一个是测试集。第一个元素是图片数据X，另一个是训练图片对应的是类别数字Y。X的大小为（60000,28,28），Y数据集大小为（60000）。

在机器学习中间，一般希望数据范围在0的附近小范围分布，因此我们将数据缩放到$[-1,1]$区间。

我们用b表示批量（batch），c表示频道数量，RGB的频道数量c=3，多长彩色图片的大小为$[b,h,w,c]$.

## 3.2 模型构建

我们要借助向量的形式：
$$
y=w^Tx+b=[w_1,w_2,w_3,...,w_{d_{in}}]·\begin{bmatrix}x_1
\\ x_2
\\ ...
\\ x_{d_{in}}
\end{bmatrix}+b
$$
通过组合多个多输入，单输出的神经元模型，可以拼成一个多输入，多输出的模型：
$$
y=Wx+b
$$
其中，$x\in R^{d^{in}},b \in R^{d^{out}},y\in R^{d^{out}},W\in R^{d^{out}×d^{in}}$.

对于多输出节点，批量训练的方式，我们将模型写成批量形式：
$$
Y=X@W+b
$$
其中，$X\in R^{b×d^{in}},b \in R^{d^{out}},Y\in R^{b×d^{out}},W\in R^{d^{in}×d^{out}}$.

符号：$x^{(1)}_1$上标表示样本的索引符号，下标表示样本向量的元素。

我们的模型只能接受向量形式的输入特征向量，因此需要把$[h,w]$的矩阵形式向量打平成$[h*w]$长度的向量。其中输入特征的长度$d_{in}=h ·w$.

对于输出标签$y$，我们将输出设置为$d_{out}$个输出节点的向量，$d_{out}$与类别个数相同。让第$i\in [1,d_{out}]$个输出节点的值表示当前样本属于类别$i$的概率$P$.如果真实标签已经唯一确定，那么如果是在第$i$类，那么索引$i$的位置为1，其他地方为0.

手写数字图片种类别有10种，输出节点数为10.一般储存时还是用数字，在计算时，根据需要来把数字编码转换成One-hot编码，通过`tf.one_hot()`即可实现。

## 3.3 误差计算

对于分类问题来说，我们的目标是最大化某个性能指标，比如准确度$acc$，但是把准确度当做损失函数去优化时，$\frac{dacc}{dθ}$其实是不可导的，无法利用梯度下降算法优化参数θ。一般的做法是设立一个平滑可导的代理函数，比如优化模型的输出$o$和one-hot编码后真实的$y$标签之间的距离。模型的训练目标是通过优化函数L来找到最优解：
$$
W^*,b^*=arg(W,b)\ minL(o,y)
$$
对于分类问题的误差计算，最常见的是采用**交叉熵（Cross Entropy）**损失函数，较少使用回归问题的均方差函数。但是之后再介绍，我们在这里仍然使用均方差损失函数求解手写数字识别问题。
$$
L(o,y)=\frac{1}{n}\sum^{n}_{i=1}\sum^{10}_{j=1}(o^{(i)}_j-y^{(i)}_j)^2
$$
我们只需要采用梯度下降法来优化损失函数来得到W和b的最优解，然后再用求得的模型去预测未知的手写数字图片。

## 3.4 真的解决了吗

现在的方案存在两大问题：

* **线性模型**：最简单的数学模型之一，参数少，计算简单，只能表达线性关系。但是图片识别比较复杂。
* **表达能力**：表达能力体现在逼近复杂分布的能力。上面的解决方案只使用少量神经元组成的一层网络模型，表达能力偏弱。

<img src="\src\images\tf001.png" style="zoom:75%;" />

## 3.5 非线性模型

我们可以给线性模型嵌套一个非线性函数，将其转换为非线性模型。我们把非线性函数称为激活函数：
$$
o=σ(Wx+b)
$$
这里的σ代表了某个具体的非线性激活函数，如Sigmoid函数，ReLu函数。

<img src="\src\images\tf002.png" alt="tf002" style="zoom:75%;" />

在这里我们通过嵌套ReLu函数将模型转换为非线性模型。

## 3.6 表达能力

针对模型表达能力偏弱的问题，可以通过重复堆叠多次变化来增加表达能力：
$$
h_1=ReLu(W_1x+b_1)
$$

$$
h_2=ReLu(W_1h_1+b_2)
$$

$$
o=W_3h_2+b_3
$$

我们将$x$称为输入层，$h$称为隐藏层，$o$是输出层。这种由大型神经元模型连接形成的网络结构称为神经网络。我们的网络模型已经变成3层神经网络，具有较好的非线性表达能力。

## 3.7 优化算法

求导数的表达式变得复杂，网络层数增加，数据的特征长度增大以及加入非线性函数后，模型表达变得复杂，很难手推公式。

因此可以使用深度学习框架，借助自动求导技术。使用起来很高效。

## 3.8 手写数字图片识别体验

### 3.8.1 网络搭建

对于第一层模型来说，输入$x\in R^{784}$，输出$h_1 \in R^{256}$设计为长度为256的向量。我们不需要显式编写$h_1=ReLu(W_1x+b_1)$计算逻辑，在TensorFlow中一行就可以实现：

```python
model = keras.Sequential([
    layers.Dense(256, activation = 'relu'),
    layers.Dense(128, activation = 'relu'),
    layers.Dense(10)
])
```

### 3.8.2 模型训练

给定输入x之后，调用`model(x)`得到模型输出$o$，通过MSE损失函数计算当前误差L：

```python
with tf.GradientTape() as tape:
    x = tf.reshape(x, (-1, 28*28))
    out = model(x)
    y_hot = tf.one_hot(y, depth = 10)
    loss = tf.square(out - y_onehot)
    loss = tf.reduce_sum(loss) / x.shape[0]
```

然后利用TensorFlow计算出导数信息：

```python
grads = tape.gradient(loss,model.trainable_variables)
```

再使用optimizers对象自动按照梯度更新法则去更新模型的参数θ。

```python
optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

多次迭代后就可以利用学好的模型预测未知的图片类型分布。测试部分暂且不讨论。

## 3.9 小结

本章提出了表达能力更强的三层非线性网络，解决手写数字识别问题。

```python
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers, datasets

(x, y), (x_val, y_val) = datasets.mnist.load_data()
x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.
y = tf.convert_to_tensor(y, dtype=tf.int32)
y = tf.one_hot(y, depth=10)
print(x.shape, y.shape)
train_dataset = tf.data.Dataset.from_tensor_slices((x, y))
train_dataset = train_dataset.batch(200)

model = keras.Sequential([
    layers.Dense(512, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dense(10)])

optimizer = optimizers.SGD(learning_rate=0.001)


def train_epoch(epoch):
    # Step4.loop
    for step, (x, y) in enumerate(train_dataset):

        with tf.GradientTape() as tape:
            # [b, 28, 28] => [b, 784]
            x = tf.reshape(x, (-1, 28 * 28))
            # Step1. compute output
            # [b, 784] => [b, 10]
            out = model(x)
            # Step2. compute loss
            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]

        # Step3. optimize and update w1, w2, w3, b1, b2, b3
        grads = tape.gradient(loss, model.trainable_variables)
        # w' = w - lr * grad
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        if step % 100 == 0:
            print(epoch, step, 'loss:', loss.numpy())


def train():
    for epoch in range(30):
        train_epoch(epoch)


if __name__ == '__main__':
    train()
```

每一个Epoch都会遍历数据集的所有样本，可以在间隔数个Epoch之后去测试模型的准确率等指标。