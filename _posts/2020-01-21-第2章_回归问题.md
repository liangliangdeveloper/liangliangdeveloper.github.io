---
layout: article
title: 第2章 回归问题
mathjax: true
articles:
  excerpt_type: html
tags: [深度学习, TensorFlow深度学习]
key: c2tensorflow
comment: true
---

## 2.1 神经元模型

神经元输入向量：$x=[x_1,x_2,x_3,...,x_n]^T$，经过函数映射$f_θ:x→y$后得到输出$y$，其中$θ$是函数自身的参数。一种简化的情况，即线性变化：
$$
f(x)=w^Tx+b
$$
展开为标量形式如下：
$$
f(x)=w_1x_1+w_2x_2+...+w_nx_n+b
$$
参数$θ=\{w_1,w_2,...,w_n,b\}$确定了神经元状态。

我们希望找到一条直线能够尽量使所有采样点到该直线的误差（Error或者损失Loss）最小。

总误差$L$的定义为观测值$wx^{(i)}+b$和真实值$y^{(i)}$之间的差的平方：
$$
L=\frac{1}{n}\sum^n_{i=1}(wx^{(i)}+b-y^{(i)})^2
$$
我们想要求$w^*,b^*$使得$L$最小。

## 2.2 优化方法

使用梯度下降法，是神经网络训练中最常使用的优化算法。使用$η$用来缩放梯度的向量。因此我们使用下面的公式进行更新：
$$
w = w - \eta \frac{dL}{dw}
$$

$$
b = b - \eta \frac{dL}{db}
$$

## 2.3 线性模型实战

我们采用的真实模型如下：
$$
y = 1.477x+0.089
$$

### 1. 采样数据

给模型添加一些误差，来自均值为0，标准差为0.01的高斯分布：
$$
y = 1.477x+0.089+ε \ ε-N(0,0.01^2)
$$

通过随机采样n=100次，获得n个样本的训练数据集$D^{train}$：

```python
data = []
for i in range(100):
    x = np.random.uniform(-10., 10.)
    eps = np.random.normal(0., 0.01)
    y = 1.477 * x + 0.089 + eps
    data.append([x, y])
data = np.array(data)
print(data)
```

### 2. 计算误差

循环计算每个点处的预测值与真实值之间的差的平方并累加，获得训练集上面的均方误差损失值。

```python
def mse(b, w, points):
    totalError = 0;
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 0]
        totalError += (y-(w * x + b)) ** 2
    return totalError / float(len(points))
```

### 3. 计算梯度

接下来我们计算梯度值：
$$
\frac{dL}{dw}=\frac{2}{n}\sum^{n}_{i=1}(wx^{(i)}+b-y^{(i)})·x^{(i)}
$$

$$
\frac{dL}{db}=\frac{2}{n}\sum^{n}_{i=1}(wx^{(i)}+b-y^{(i)})
$$

实现的代码如下：

```python
def step_gradient(b_current, w_current, points, lr):
    b_gradient = 0
    w_gradient = 0
    M = float(len(points))
    for i in range(0, len(points)):
        x = point[i, 0]
        y = point[i, 1]
        b_gradient += (2/M) * ((w_current * x + b_current) - y)
        w_gradient += (2/M) * x * ((w_current * x + b_current) - y)
    new_b = b_current - (lr * b_gradient)
    new_w = w_current - (lr * w_gradient)
    return [new_b, new_w]
```

### 4. 梯度更新

计算出w和b的梯度之后，就可以更新w和b的值。我们把对数据集的所有样本训练一次成为一个Epoch，一共迭代num_iterations个Epoch，实现如下：

```python
def gradient_descent(points, starting_b, starting_w,lr, num_iterations):
    b = starting_b
    w = starting_w
    for step in range(num_iterations):
        b, w = step_gradient(b, w, np.array(points), lr)
        loss = mse(b, w, points)
        if step%50 == 0:
            print(f"iteration:{step}, loss:{loss}, w:{w}, b:{b}")
    return [b, w]
```

### 主函数

主函数代码如下：

```python
def main():
    lr = 0.01
    initial_b = 0
    initial_w = 0
    num_iterations = 1000
    [b,w] = gradient_descent(data, initial_b, initial_w, lr, num_iterations)
    loss = mse(b, w, data)
    print(f"Final loss:{loss}, w:{w}, b:{b}")
```

运行结果如下：

```
iteration:0, loss:7.718815664445674, w:0.9975333227285473, b:-0.01978728156035148
iteration:50, loss:0.0020120219538275665, w:1.476374077875862, b:0.0441964562709057
iteration:100, loss:0.0003788396226925859, w:1.4769876965789543, b:0.07181156697176126
iteration:150, loss:0.00015496614572430608, w:1.4772148831068257, b:0.08203580082463914
iteration:200, loss:0.00012427800225472868, w:1.4772989967725048, b:0.08582122681878866
iteration:250, loss:0.00012007133086889436, w:1.4773301390625881, b:0.08722274503163248
iteration:300, loss:0.00011949468848692595, w:1.4773416692012855, b:0.08774164390958414
iteration:350, loss:0.00011941564346387036, w:1.4773459381262113, b:0.0879337613170298
iteration:400, loss:0.00011940480812490968, w:1.4773475186553358, b:0.08800489097217443
iteration:450, loss:0.00011940332283758809, w:1.4773481038313125, b:0.08803122605501919
iteration:500, loss:0.00011940311923727376, w:1.4773483204871898, b:0.08804097637065088
iteration:550, loss:0.00011940309132813635, w:1.4773484007019815, b:0.08804458633290081
iteration:600, loss:0.00011940308750240474, w:1.4773484304007505, b:0.08804592288732
iteration:650, loss:0.0001194030869779816, w:1.477348441396439, b:0.08804641773396711
iteration:700, loss:0.00011940308690609432, w:1.4773484454674888, b:0.0880466009462633
iteration:750, loss:0.00011940308689623812, w:1.4773484469747566, b:0.08804666877888515
iteration:800, loss:0.00011940308689489167, w:1.4773484475328083, b:0.08804669389327253
iteration:850, loss:0.00011940308689470363, w:1.4773484477394216, b:0.08804670319163727
iteration:900, loss:0.00011940308689467952, w:1.4773484478159182, b:0.0880467066342689
iteration:950, loss:0.00011940308689467389, w:1.4773484478442402, b:0.08804670790887077
Final loss:0.00011940308689467549, w:1.4773484478546026, b:0.08804670837521167
```

## 2.4 线性回归

给定数据集D，我们需要从D中学习新的模型，从而预测出没有见过的样本输出值。对于预测值是连续的实数范围，或者属于某一段连续的实数区间，我们把这一类问题称为回归（Regression）问题。如果用线性模型逼近真实模型，这一类方法称为线性回归（Linear Regression，LR）。

接下来我们将挑战分类问题。