---
layout: article
title: 第2章 爬虫基础
mathjax: true
articles:
  excerpt_type: html
tags: 爬虫
key: c2pc
comment: true
---

# 第2章 爬虫基础

## 2.1HTTP基本原理

详细了解HTTP原理，理解浏览器中敲入URL之后发生了什么。

### 2.1.1 URI和URL

**URI：**Uniform Resource Identifier，统一资源标志符

**URL：**Universal Resource Locator，统一资源定位符

例如https://github.com/favicon.ico是GitHub的图标连接，它是一个URL，也是一个URI，有这样一个图标资源，我们用URL/URI指定了唯一的访问方式。这其中包括https协议，访问路径和资源名称。

URL是URI的子集，子集里面还有URN（Universal Resource Name，统一资源名称），只命名资源但是不指定如何定位资源。但是目前网上基本上全是URL，URN很少见。

### 2.1.2 超文本

 超文本（hypertext）。我们在网页中看到的网页就是超文本组成的，网页源代码是一系列HTML代码，包含一系列标签，`img`显示图片，`p`指定显示段落。解析这些标签后就变成了我们看到的网页。网页源代码HTML可以称为超文本。

### 2.1.3 HTTP和HTTPS

HTTP全称Hyper Text Transfer Protool ，超文本传输协议。适用于从网络传输超文本数据到本地浏览器的传送协议，保证高效准确的传输超文本文档。

HTTPS全称Hyper Text Transfer Protool over Secure Socket Layer。是以安全为目标的HTTP通道，是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。
HTTPS的安全基础是SSL，通过他传输的内容都是经过SSL加密的，主要作用分为两种：
> 建立一个信息安全通道来保证数据的安全。
> 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。
### 2.1.4 HTTP请求过程
浏览器中输入一个URL回车后便会在浏览器中观察到页面内容，这实际上是浏览器向网站所在服务器发送了一个请求，服务器接收到请求进行处理和解析，返回对应的响应，接着传回浏览器，响应里面包含源代码等内容。浏览器进行解析就能显示出来。
Chrome浏览器开发者模式下的Network监听组件可以显示访问当前请求网页时发生的请求和响应。
每一列含义如下：
**第一列Name**：请求名称，一般会用URL的最后一部分当作名称。
**第二列Status**：响应的状态码，显示为200表示响应正常。通过状态码判断发送请求后是否得到正常响应。
**第三列Type**：请求的文档类型。document意思是请求了一个HTML文档，内容是HTML代码。
**第四列Initiator**：请求源。标记请求是由哪个对象或进程发起的。
**第五列Size**：从服务器下载的文件和请求的资源大小。如果是缓存中请求的资源，则该列会显示 from cache。
**第六列Time**：发送请求到获取响应时间所用的总时间。
**第七列Waterfall**：网络请求的可视化瀑布流。
点击这个条目可以看到更详细的信息：
General部分：Request URL为请求的URL，Request Method为请求的方法，Status Code为状态响应码，Remote Address为远程服务器的地址和端口，Referrer Policy为Referrer判别策略。
Response Headers和Request Headers分别代表响应头和请求头。头中带有很多信息，例如浏览器标识，Cookies，Host等信息，这是请求的一部分，服务器会根据请求头内信息判断请求是否合法。做出对应的响应。Response Header就是响应的一部分，包含了服务器类型，文档类型，日期等信息，浏览器接受响应后会解析响应内容。
### 2.1.5 请求
请求由服务器发出，可以分为4部分内容，请求方法（Request Method），请求的网址（Request URL），请求头（Request Headers），请求体（Request Body）。
#### 1.请求方法
常见方法有两种：GET和POST。
在浏览器直接舒服URL回车就是发送了一个GET请求。请求参数直接包含到URL中。POST请求大多在表单提交时发起，例如登陆。数据以表单形式传输，而不会体现在URL中。
GET和POST的区别：
> GET参数包含在URL，可以直接看到，POST请求不包含，是以表单形式传输。
> GET请求提交的数据最多只有1024字节，而POST没有限制。

敏感信息和上传大文件会选用POST方式。
#### 2.请求的网址
即URL，唯一确定我们想要的资源。
#### 3.请求头
说明服务器使用的附加信息，比较重要的信息有Cookie，Referrer，User-Agent等。
> **Accept：**请求报头域，用于指定客户端能接受的信息。
> **Accept-Language：**指定客户端可接受的语言类型。
> **Accept-Encoding：**指定客户端可接受的内容编码。
> **Host：**指定请求资源的主机IP和端口号，内容为请求URL原始服务器或网关的位置。
> **Cookie：**辨别用户进行会话跟踪而储存在用户本地的数据。主要功能是维持当前访问会话。（保持登陆）
> **Referrer：**标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息作相应处理，如来源统计，防盗链统计。
> **User-Agent：**：UA，特殊的字符串头，识别客户使用的操作系统及版本，浏览器及版本信息。爬虫时加上可以伪装浏览器，不加可能会被识别为爬虫。
> **Content-type：**也叫互联网媒体类型（Internet Media Type）或者MIME类型。在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如txt/html代表HTML格式，image/gif代表图片。

请求头是请求的重要组成部分，爬虫的时候大部分情况下都要设定请求头
#### 4.请求体
承载的内容为POST请求中的表单数据，对于GET请求请求体为空。
在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各个请求库各个参数设置时使用的是哪种Content-type否则会导致无法响应。
### 2.1.6 响应
由服务器返回客户端，可以分为三部分：响应状态码（Response Status Code），响应头（Response Header），响应体（Response Body）。
#### 1.响应状态码
表示服务器响应状态：200代表正常响应，404代表页面未找到，500代表服务器内部发生错误。
#### 2.响应头
> **Date：**标识响应的时间。
> **Last-Modified：**指定资源的最后修改时间。
> **Content-Encoding：**指定响应内容的编码。
> **Server：**包含服务器信息，比如名称，版本号。
> **Content-type：**文档类型，指定返回的数据类型是什么。
> **Set-Cookie：**设置Cookie，Set-Cookie告诉浏览器要将此内容放入Cookie中，下次请求携带Cookie请求。
> **Expires：**指定响应过期时间，可以使代理服务器或浏览器将加载的内容更新在缓存中。再次访问时从缓存中加载，降低服务器负载，缩短加载时间。

#### 3.响应体
响应的正文数据都在响应体中，比如请求网页时，他的响应体就是HTML代码。浏览器开发者工具中点击Preview就可以看到网页源代码，也就是响应体的内容。

## 2.2 网页基础
### 2.2.1 网页的组成
网页可以分为3大部分——HTML，CSS和Javascript。HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤。
#### 1.HTML
描述网页的一种语言，全称Hyper Text Markup Language。超文本标记语言。不同类型文字用不同标签表示，例如图片用img，视频用video，段落用p。他们之间的布局又常通过布局标签div嵌套组合而成。
#### 2.CSS
全称Cascading Style Sheets，即层叠样式表，“层叠”指的是当HTML引用了多个CSS文件，并且样式发生冲突时，浏览器依照层叠顺序处理。“样式”指的是网页中文字的格式。
CSS是目前唯一的网页页面排版样式标准。
在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中。在HTML中只需link标签即可引入写好的CSS文件。
#### 3.JavaScript
简称JS，是一种脚本语言。提供交互和动画效果。实现实时动态交互的页面功能。
以单独文件的形式加载，后缀为js，在HTML标签中通过script标签即可引用。
### 2.2.2 网页的结构
```html
<! DOCTYPE html>
<html>
<head>
<meta charset=“UTF-8”>
<title> This is a Demo. </title>
</head>
<body>
<div id=“container”>
<div class=“wrapper”>
<h2 class=“title”>Hello World</h2>
<p class=“text”>Hello, this is a paragraph.</p>
</div>
</div>
</body>
</html>
```
开头用DOCTYPE定义了文档类型，最外层是html标签，还有一个对应的结束标签表示闭合。内部是head标签和body标签，分别代表网页头和网页体，也需要结束标签。head内部定义了一些页面的配置和引用，例如\<meta  charset=“UTF-8”>指定了网页编码为UTF-8.
title定义了文章的标题，会出现在选项卡中，不会出现在正文中。body是正文中显示的内容。div定义了网页的区块，id的内容在网页是唯一的。class常用来和class配合设定样式。h2代表二级标题。p代表段落。他们都有各自的class属性。
### 2.2.3 节点树及节点间的关系
HTML中，所有标签定义的内容都是节点，构成了一个HTML DOM树。
什么是DOM？W3C的标准，全称Document Object Model，即文档对象模型。定义了访问HTML和XML的文档标准。
中立于平台和语言的接口，它允许程序和脚本动态的访问和更新文档的内容，结构和样式。
分为3个不同部分：
> **核心DOM：**针对任何结构化文档的标准模型。
> **XML DOM：**针对XML文档的标准模型。
> **HTML DOM：**针对HTML文档的标准模型。

根据HTML DOM标准，HTML文档中所有内容都是节点。
> 整个文档是一个文档节点
> 每个HTML元素是元素节点
> HTML元素内文本是文本节点
> 每个HTML属性是属性节点
> 注释是注释节点

将HTML文档视为树结构，这种结构被称为节点树。通过HTML DOM，树中所有节点均可通过JS访问，所有HTML节点都可被修改，创建或删除。
彼此拥有层级关系。
### 2.2.4 选择器
网页由一个个节点组成，CSS选择器会根据不同的节点设置不同的样式规则，那么怎样定位节点？
在CSS中使用CSS选择器定位节点。例如上个例子div节点的id为container，那么就可以表示为#container，#表示选择的id，想选择class为wrapper的节点，就用.wrapper，这里点（.）开头代表选择class。还可以根据标签筛选，例如想选择二级标题直接用h2即可。
同时CSS支持嵌套选择。各个选择器之间加上空格分隔开就是嵌套关系。不加空格表示并列关系。
CSS最常用的选择器是XPath。
## 2.3 爬虫的基本原理
### 2.3.1 爬虫概述
爬虫就是获取网页并提取和保存信息的自动化程序。
#### 1.获取网页
获取网页的源代码，Python提供了许多库帮我们实现发送请求的操作。如urllib，requests等。我们可以使用这些库实现HTTP请求操作。得到源代码后只需要解析body部分即可。
#### 2.提取信息
通用方法是正则表达式提取，万能的方法。但是复杂的正则表达式容易出错。
由于网页结构有一定规则，还有一些网页节点的属性，CSS选择器或XPath来提取网页信息的库，如Beautiful Soup，pyquery，lxml等等。用这些库可以快速提取网页信息，如节点属性，文本值。

#### 3.保存数据

保存形式多种多样，可以简单保存为TXT或者JSON文本，也可以保存在数据库如MySOL和MongoDB等，也可保存到远程服务器，如借助SFTP进行操作。

#### 4.自动化程序

爬虫就是代替我们来完成爬取工作的自动化程序。他可以在抓取过程中进行各种异常处理，错误重试等操作。确保爬虫高效运行。

### 2.3.2 能抓怎样的数据

最常抓取的是HTML代码。

另外，有些网页返回的可能不是HTML代码，而是一个JSON字符串（API接口大多采用这种形式），这种格式方便传输和解析，同样可以抓取，而且数据提取更方便。

可以爬取二进制数据（图片，视频，音频），保存成对应文件名。

还可以看到各种扩展名，如CSS，JavaScript和配置文件等，也是普通文件，只要在浏览器能访问的到，就可以抓取下来。

上述内容都对应着各自的URL，是基于HTTP或者HTTPS协议的，只要是这种数据，爬虫都可以抓取。

### 2.3.3 JavaScript渲染页面

有时候用urllib或者request抓取页面时，源代码和浏览器看到不一样。现在越来越多的网页采用Ajax、前段模块化工具来构建，整个网页都能用JavaScript渲染出来，原始HTML代码就是一个空壳。
```html
<! DOCTYPE html>
<html>
<head>
<meta charset=“UTF-8”>
<title> This is a Demo. </title>
</head>
<body>
<div id=“container”>
</div>
</body>
<script src="app.js"></script>
</html>
```

body里只有一个id为container的节点，但是在body节点后引入了app.js。负责页面的渲染。

浏览器打开这个页面，会先加载HTML内容，接着浏览器会发现引入了app.js文件，然后会去请求这个文件，获到文件后，会执行JS代码，JS会改变HTML中的节点，向其添加内容，最后得到完整界面。

因此我们用urllib或者request等库请求当前页面时，只是得到了HTML代码，不会继续加载JS文件，看不到浏览器内容。

因此，使用基本HTTP请求库得到的代码可能和浏览器中的源代码不一样。对于这样的情况，我们可以分析后台Ajax接口，可以使用Selenium、Splash这样的库来模拟JS渲染。

## 2.4 会话和Cookies

### 2.4.1 静态网页和动态网页

前面的示例代码保存为.html文件，放在某台具有固定公网IP的主机上，主机上装上Apache或Nginx等服务器，这样这台主机就能作为服务器了，其他人可以通过访问服务器看到这个页面，这就搭建了最简单的网站。

这种网站内容是HTML编写，文字图片都由HTML指定，这种叫做静态网页，编写简单，可维护性差，不能根据URL灵活变换显示内容。例如传入一个name参数，让其在网页显示出来无法做到。

动态网页应运而生，可以动态解析URL中的参数变化，关联数据库并动态呈现不同页面内容。它们不再是简单的HTML，可能是由JSP，PHP，Python等语言编写的，功能比静态网页强大丰富多了。

此外，动态网页还可以实现用户登录和注册功能。

### 2.4.2 无状态HTTP

HTTP的无状态是指HTTP协议对事务处理没有记忆功能，服务器不知道客户端是什么状态。服务器不会记录前后状态变化，缺少状态记录。这就意味着如果后续需要处理前面的信息，必须重传。这导致需要额外传递一些前面的重复请求，才能获取后续响应。为了保持前后状态，我们不能把前面的请求重传一次，浪费资源，尤其是需要用户登陆的。

两个用于保持HTTP连接状态技术出现了。分别是会话和Cookies。会话在服务端，也就是服务器，用来保存用户的会话信息；Cookies在客户端，有了Cookies，浏览器在下次访问网页会自动附带上它发送给服务器。服务器识别Cookies鉴别是哪个用户，再判断用户是否是登录状态，返回对应的响应。

因此在爬虫时，有时候处理登录才能访问的页面时，一般会直接将登陆成功获取的Cookies放在请求头直接请求，不必重新模拟登录。

#### 1.会话

本来含义是有始有终的一系列动作/消息。在Web中，会话对象用来储存特定用户会话所需的属性及配置信息。当用户请求来自应用程序的Web页时，如果该用户还没有会话，则Web服务器将自动创建一个会话对象，当会话过期或被放弃之后，服务器将终止该对话。

#### 2.Cookies

指某些网站为了辨别用户身份、进行会话跟踪而储存在本地终端的数据。

#### ● 会话维持

怎么利用Cookies保持状态？

客户端第一次请求服务器，服务器会返回一个请求头中带有`Set-Cookies`字段响应给客户端，用来标记是哪个用户。客户端会把Cookies保存起来。下一次请求的时候，浏览器会把Cookies放在请求头一起交给服务器。Cookies携带了会话ID信息，服务器检查Cookies可找到响应会话，再判断会话状态。

在成功登录某个网站时，服务器会告诉客户端设置哪些Cookies信息，在后续访问页面时客户端会把Cookies发送给服务器，服务器找到对应的会话加以判断，如果某些设置登录状态是有效的，那证明处于登录状态，此时返回登录后才能查看的内容，浏览器进行解析就能看到。

反之，如果传给服务器的Cookies是无效的或者会话已经过期，那么我们就不能访问页面，此时会受到错误的响应或者重新登录。

Cookies和会话需要配合，二者共同协作。

#### ● 属性结构

> **Name：**该Cookies的名称，一旦创建不可修改。
>
> **Value：**该Cookies的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。
>
> **Domain：**可以访问该Cookies的域名。
>
> **Max Age：**该Cookies的失效时间，单位为秒，常和Expires使用。通过它可以计算出有效时间。如果为整数，那么Cookies将会在Max Age后失效，如果为负数，则关闭浏览器时Cookies即失效。
>
> **Path：**该Cookies使用的路径，如果设置为/path/，则只有路径为/path/的路径才能访问该Cookies；如果设置为/，本域名下任何页面都能访问该Cookies。
>
> **Size字段：**Cookies的大小
>
> **HTTP字段：**Cookies的httponly属性。如果此属性为true，则只有在HTTP头中带有此Cookies信息，而不能通过document.cookies来访问该Cookies。
>
> **Secure：**Cookies是否使用安全协议传输。安全协议有HTTPS和SSL等，在网络传输之前先将数据加密，默认为false。

#### ● 会话Cookie和持久Cookie

会话Cookie就是把Cookie放在浏览器内存里，浏览器关闭，该Cookie失效；持久Cookie则会保存在硬盘里，下次还能继续使用，长久保持用户状态。

严格来说这两个不区分，是靠Max Age和Expires决定过期时间。一些持久化登录界面会把的Cookies的有效世界和会话的有效期设置的比较长。

### 2.4.3 常见误区

误区：只要关闭浏览器，会话就消失了。

关闭浏览器之前，不会通知服务器他会关闭。有这种错觉是因为大部分会话机制都使用会话Cookie来保存会话信息，关闭浏览器之后，Cookie消失。再次连接服务器，无法找到原来的会话。如果Cookie保存在硬盘上，再次打开浏览器还能找到会话，保持登录。因此服务器要设置会话失效时间，服务器认为客户端停止活动，才会把会话删除节省储存空间。

## 2.5 代理的基本原理

最初爬虫正常运行，但是后来出现“您的IP访问频率太高”提示，是因为反爬虫机制。封IP。

### 2.5.1 基本原理

代理实际上就是代理服务器，proxy server，功能是代理网络用户去取得网络信息。它是信息的中转站，我们正常请求一个网站的时候，是发送了请求给Web服务器，Web服务器把相应传回给我们。设置代理服务器，就是在本机和服务器之间搭建了一个桥，此时本机不是直接向Web服务器发送请求，而是向代理服务器发送请求，然后由代理服务器再发送给Web服务器。接着由代理服务器把响应发送给本机。同样可以访问网页，但是这个时候IP就不是本机的IP了，实现了IP伪装。

### 2.5.2 代理的作用

* 突破自身IP限制，访问不能访问的网站
* 访问单位或者团体资源
* 提高访问速度
* 隐藏真实IP。防止IP封锁

### 2.5.3 爬虫代理

使用代理隐藏真实IP，让服务器误以为是代理服务器在请求自己，在爬取过程中不断更换代理，就不会被封锁，可以达到很好的爬取效果。

### 2.5.4 代理分类

**1.根据协议区分**

* FTP代理服务器：用于访问FTP服务器，一般有上传，下载，缓存功能，端口一般为21、2121等
* HTTP代理服务器：用于访问网页，一般有内容过滤和缓存功能。端口一般为80、8080、3128等
* SSL/TLS代理：一般用于加密网站，一般有SSL和TLS加密功能，端口一般为443
* RTSP代理：用于访问Real流媒体服务器，一般有缓存功能，端口一般为554
* Telnet代理：用于Telnet远程控制（黑客入侵计算机常用隐藏身份），端口一般为23
* POP3/SMTP代理：用于POP3/SMTP方式收发邮件，一般有缓存功能，端口一般为110/25
* SOCKS代理：只是单纯传输数据包，不关心具体协议和方法，速度快很多，一般有缓存功能，端口一般是1080

**2.根据匿名程度区分**

* 高度匿名代理
* 普通匿名代理
* 透明代理
* 间谍代理

### 2.5.5 常见代理设置

* 使用网上的免费代理
* 使用付费的代理服务
* ADSL拨号：拨一次号换一次IP