---
layout: article
title: Neural Networks and Deep Learning
mathjax: true
articles:
  excerpt_type: html
tags: 深度学习
key: w1deeplearn
comment: true
---

# 深度学习笔记 第一课 Neural Networks and Deep Learning

## 第一周

### What is a neural network？

**神经网络**：神经元里面是一个函数，有输入和输出。

**ReLU函数**：线性整流函数（Rectified Linerar Unite）

大型的神经网络是由多个神经元组成的。

x-input y-output

**隐藏神经元*

### Supervised Learning with Neural Networks

**监督学习**：在监督学习中，你有一些输入x并且你想通过x学习一个函数，这个函数能够将x映射到y。

Standard NN

CNN：图像

RNN：序列

**Structured Data**：数据化结构是基于数据库的数据

**Unstructured Data**：音频，图片，文本

### Why is Deep Learning taking off

Scale drives deep learning progress

1.训练足够大的神经网络

2.有足够多的数据

m：训练集的大小

* Data
* Computation
* Algorithm：sigmoid向ReLU函数的转移

Idea->Code->Experiment->Idea......

### About this Course

Week1: Introduction

Week2: Basics of Neural Network programming

Week3: One hidden layer Neural Networks

Week4: Deep Neural Networks

## 第二周

### Binary Classification

**逻辑回归算法**：二元分类算法

图像在电脑的表示：三维矩阵（RGB）展开成一个一维向量，维度用n表示。

训练集：$(x,y)\quad x\in R^{n_x},y \in \{0,1\}$
$$
m\quad training\quad example:\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}
$$
将训练集写的更紧凑：
$$
X=[x^{(1)}\quad x^{(2)}...x^{(m)}](n_x*m)
$$

$$
Y = [y^{(1)}\quad y^{(2)}...y^{(m)}](1*m)
$$

### Logistic Regression

给出x，我们希望得到y，y是一个预测值，我们称之为yhat(ŷ)，$ŷ=P(y=1|x)$
$$
x \in R^{n_x}
$$

$$
w \in R^{n_x} \quad b \in R
$$

$$
output: y^{hat}=\sigma(w^Tx+b)
$$

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$

### Logistic Regression Cost Function

Loss（损失）函数：$L(y^{hat},y)=-(ylogy^{hat}+(1-y)log(1-y^{hat}))$

拉斐尔效应

Loss函数用于单一情况。

Cost（代价）函数：$J(w,b)=\frac{1}{m}\sum ^{m}_{i=1}L(y^{hat(i)},y^{(i)})$

Cost函数检验参数成本

### Gradient Descent

梯度下降模型

找到w和b使得$J(w,b)$尽可能的小。

初始化w，b。一般用0初始化。
$$
w:=w-learning \quad rate*dw(dw=\frac{dJ(w,b)}{dw})
$$

$$
b:=b-learning \quad rate*db(db=\frac{dJ(w,b)}{db})
$$

### Logistic Regression Gradient Descent

$$
da=-\frac{y}{a}+\frac{1-y}{1-a}
$$

$$
dz=a-y
$$

$$
dw_i=x_i*dz
$$

$$
db=dz
$$

### Gradient Descent on m Examples

```c
J=0;dw1=0;dw2=0;db=0;
for i=1 to m {
    z[i]=w.T*x[i]+b;
    a[i]=sigmoid(z[i]);
    J+=-[y[i]log(a[i])+(1-y[i])log(1-a[i])];
    dz[i]=a[i]-y[i];
    dw1+=x1[i]*dz[i];
    dw2+=x2[i]*dz[i];
    db+=dz[i];
    J/=m;
    dw1/=m;dw2/=m;db/=m;
}
w1:=w1-learning_rate*dw1;
w2:=w2-learning_rate*dw2;
b:=b-learning_rate*db;
```

存在着两个for循环，1）for循环从1到m个训练集；2）循环了w1到wn一共n个特征。

解决方式：**向量化**

### Vectorization

**向量化**

计算向量：z=np.dot(w,X)

**SIMD**：并行化计算

### More Vectorization Examples

```c
J=0;dw=np.zeros(n_x,1);db=0;  //dw改为向量形式
for i=1 to m {
    z[i]=w.T*x[i]+b;
    a[i]=sigmoid(z[i]);
    J+=-[y[i]log(a[i])+(1-y[i])log(1-a[i])];
    dz[i]=a[i]-y[i];
    dw+=x[i]*dz[i]            //向量化的乘法计算
    db+=dz[i];
    J/=m;
    dw1/=m;dw2/=m;db/=m;
}
w1:=w1-learning_rate*dw1;
w2:=w2-learning_rate*dw2;
b:=b-learning_rate*db;
```

### Vectorizing Logistic Regression

逻辑回归的向前传播：X定义为所有训练值

`Z=np.dot(w.T,X)+b`，b使用了广播机制

### Vectorizing Logistic Regression‘s Gradient Output

定义：$dZ=[dz^{(1)}\quad dz^{(2)} \quad ... \quad dz^{(m)}]$
$$
dZ=A-Y
$$
dw,db的处理：

```python
db=1/m*np.sum(dZ)
dw=1/m*X*dZ.T
```

### Broadcasting in Python

```python
cal = A.sum(axis=0)
```

`axis=0`是垂直求和，`axis=1`是水平求和。

<img src="\src\images\1575517503767.png" alt="1575517503767" style="zoom:50%;" />

**General Principle**：

<img src="\src\images\1575517948105.png" alt="1575517948105" style="zoom:60%;" />

### A note on python/numpy vectors

不要使用秩为1的数组（shape为(n,)没有列长，转置无效）

```python
a = np.random.randn(5,1)
a = np.random.randn(1,5)
assert(a.shape == (5,1))
```

> `assert`的备注：
>
> assert后面的条件如果不成立，会立即报错，帮助debug。

## 第三周

### Neural Networks Overview

**快速了解如何实现神经网络：**

有一个类似a和z的计算，先算z再算a。

**记号：**

上标表示节点的层数：$W^{[1]}$

### Neural Network Representation

<img src="\src\images\1576477370461.png" alt="1576477370461" style="zoom:100%;" />

从左到右：**Input Layer，Hidden Layer，Output Layer**

**隐藏层：**指在训练集中，中间这一层节点的真实值并没有所观察

输入层$a^{[0]}=X$,a指的是activation（激活）

下标表示层中的第几个节点

计算层数时，计入输入层，但是编号1从隐藏层开始。

### Computing a Neural Network's Output

![1576477747077](\src\images\1576477747077.png)

逻辑回归的圆代表两步计算。
$$
a^{[l]←layer}_{i←node \  in \  layer}
$$
![1576478004008](\src\images\1576478004008.png)

向量化：
$$
z^{[1]}=W^{[1]}x+b
$$

$$
a^{[1]}=\sigma(z^{[1]})
$$

各个层的大小：$z={[n,1]},W=[n,m],x=[m,1],b=[n,1]$

### Vectorizing across multiple examples

$$
a^{[2](i)←i代表训练实例}
$$

消除训练实例i的for循环。
$$
Z^{[1]}=W^{[1]}X+b^{[1]}
$$

$$
A^{[1]}=\sigma (Z^{[1]})
$$

$$
X=[x^{(1)} \ x^{(2)} \ ... \ x^{(m)}](n_x,m)
$$

横向代表训练实例，纵向代表隐藏层节点数

### Explanation for Vectorized Implementation

![1576544947900](\src\images\1576544947900.png)

### Activation functions

$$
a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
$$

具有居中数据效果。tanh比sigmoid更优
$$
a=ReLu(z)=max(0,z)
$$
二元分类：sigmoid，输出层可以用

其他：ReLu，不确定的时候用，大部分z空间斜率斜率不为0.

### Why do you need non-linear activation functions?

为什么要使用非线性激活函数？

![1576545714186](\src\images\1576545714186.png)

神经网络的输出只是线性的变化，无论使用多少层，都是一个线性的输出。隐藏层无用。

如果机器学习用于回归问题，那么可以在输出层用线性激活函数。

### Derivatives of activation functions

**sigmoid函数**：$\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))$

**tanh函数：**$\frac{d}{dz}g(z)=1-(tanh(z))^2=1-g^2(z)$

**RuLu函数：**$\frac{d}{dz}g(z)= \ 0(z<0) \ 1(z>=0)$

**Leaky ReLu函数：**$\frac{d}{dz}g(z)= \ 0.01(z<0) \ 1(z>=0)$

### Grandient descent for Nerual Networks

参数：$W^{[1]}(n^{[1]},n^{[0]}),b^{[1]}(n^{[1]},1),W^{[2]}(n^{[2]},n^{[1]}),b^{[2]}(n^{[2]},1)$

$n_x=n^{[0]},n^{[1]},n^{[2]}=1$

Cost Function:$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac{1}{m} \sum^n_{i=1}L(y_{hat},y)$

计算dW和db：
$$
dZ^{[2]}=A^{[2]}-Y
$$

$$
dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}
$$

$$
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims = true)
$$

$$
dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{'[1]}(Z^{[1]})
$$

***乘法是对应位置相乘**
$$
dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T
$$

$$
db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis = 1.keepdims=true)
$$

### Random Initialization

神经网络初始化为0无用。同一层的激活函数相同，反向传播时用同样的方式输出权重。这是对称的。实现了完全相同的单元。

使用：`np.random.randn((2,2))*0.01`初始化w，b可以用0（`np.zeros`）初始化。乘一个很小的数的原因是使得梯度下降不缓慢，数字太大会导致学习速度缓慢。

## 第四周

### Deep L-layer neural network

逻辑回归是一个比较浅的神经网络。浅和深只是程度的区别。

很难实现得知需要多少网络。

深度网络使用的符号：

![1576650208462](\src\images\1576650208462.png)

这是四层神经网络，3个隐藏层。

$L=4$（大写L代表层数），$n^{[l]}$表示第l层的节点数，输入层是第0层。$a^{[l]}$表示第l层的激活函数。

$a^{[l]}=g^{[l]}(z^{[l]})$,$w^{[l]}$,$b^{[l]}$.
$$
a^{[0]}=x,a^{[L]}=y_{hat}
$$

### Forward Propagation in a Deep Network

向前传播通用规则：
$$
z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}
$$

$$
a^{[l]}=g^{[l]}(z^{[l]})
$$

向量化:
$$
Z^{[l]}=w^{[l]}A^{[l-1]}+b^{[l]}
$$

$$
A^{[l]}=g^{[l]}(Z^{[l]})
$$

每一层用for循环不断加l的值，这无法避免。

### Getting your matrix dimensions right

![1576651069981](\src\images\1576651069981.png)

$w^{[l]}$的维度为$(n^{[l]},n^{[l-1]})$,$b^{[l]}$的维度为$(n^{[l]},1)$。$dw,db$和$w,b$维度一样。

向量化：

w，b保持不变，反向传播时$dZ,dA$的维度和$Z,A$一样。$Z,A:(n^{[l]},m)$

### Why deep representations?

为什么深度很有效？

人像识别：

图像->边缘识别->特征组合检测部位->检测整张脸->输出不同类型脸部。

非正式地 我们用相对较小但具有深度的神经网络来计算这些功能 而这里的小 是说隐藏神经元的数量相对较小 但是 如果我们尝试使用浅网络计算相同的功能 它没有足够多的隐藏层 然后 我们可能需要更多的指数级的隐藏神经元来进行计算

### Building blocks of deep neural networks

对于某一层来说：

$layer\ L:w^{[l]},b^{[l]}$

$→Forward:input-a^{[l-1]},output-a^{[l]}$

$z^{[l]}:w^{[l]}a^{[l-1]}+b^{[l]},\ cache\ z^{[l]}$

$a^{[l]}:g^{[l]}(z^{[l]})$

$→Backward:input-da^{[l]}(cache\ z^{[l]}),output-da^{[l-1]}(dw,db)$

![1576652649132](\src\images\1576652649132.png)

### Forward and Backward Propagation

![1576653708252](\src\images\1576653708252.png)

### Parameters vs Hyperparameters

参数：$w \ \& \ b$

超参数：学习率**α**，迭代次数，隐藏层数L，隐藏层单元数，激活函数......

尝试不同的取值，查看效果如何，更改我们的参数。

### What does this have to do with the brain?

No Connection！