---
layout: article
title: 第3章 基本库的使用
mathjax: true
articles:
  excerpt_type: html
tags: 爬虫
key: c3pc
comment: true
---

# 第三章 基本库的使用

## 3.1 使用urllib

Python3中，urllib库被统一。urllib库是Python内置的HTTP请求库。不需要额外安装使用。包含以下4个模块：

> `request`：最基本的HTTP模块，用来模拟发送请求。就像在浏览器里输入网址回车一样。只要给库方法传入URL和额外的参数，就可以模拟实现这个过程。
>
> `error`：异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。
>
> `parse`：一个工具模块，提供了许多URL处理方法，比如拆分，解析，合并。
>
> `robotparser`：主要是处理网站的robots.txt文件，然后判断哪么网站可以爬，哪些不可以爬，用的比较少。

### 3.1.1 发送请求

使用urllib的`request`模块，可以方便的实现请求的发送并得到响应。接下来就是具体用法：

#### 1.urlopen()

`urllib.request`提供了最基本的构造HTTP请求方法，利用它可以模拟浏览器请求发起过程，同时还有处理授权验证（authentication）、重定向（redirection）、浏览器Cookie以及其他内容。

```python
import urllib
import urllib.request      #Python3不支持自动导入urllib的底层包

responce = urllib.request.urlopen('https://www.python.org')
print(responce.read().decode('utf-8'))
```

运行结果如下所示：

![1573125322801](\src\images\1573125322801.png)

用两行代码爬取了网页的源代码。利用type()方法获得相应类型。

```python
print(type(responce))
```

输出结果如下：

```
<class 'http.client.HTTPResponse'>
```

是一个HTTPResponse类型对象，包含`read()`,`readinto()`,`getheader(name)`（获得相应name对应的响应头的值）,`getheaders()`（获得相应头信息）,`fileno()`等方法。以及`msg`,`version`,`status`（获得状态码）,`reason`,`debuglevel`,`closed`等属性。

最基本urlopen()方法，可以完成最基本的简单网页的GET请求抓取。接下来看urlopen()的API：

```python
urllib.request.urlopen(url, data=None, [time,]*, cafile=None, capath=None, cadefault=False, context=None)
```

下面是几个参数用法：

**data参数**

如果要添加该参数，并且如果它是字节流编码格式的内容，即bytes类型，需要通过bytes()转化。传递了该参数，请求就不是GET方式，而是POST方式。

实例如下：

```python
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
responce = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(responce.read())
```

这里我们传递了一个参数word，值为hello。它需要被转码为bytes（字节流）类型。其中转字符流用了bytes（）方法，该方法的第一个参数需要的是str类型，需要用urllib.parse模块里的urlencode()方法来将参数字典转为字符串；第二个参数指定编码格式，这里指定为utf8.

这里请求的网站为httpbin.org，它可以提供HTTP请求测试，我们请求的链接就可以来测试POST请求，可以输出一些信息，包含我们传递的data参数。

```
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "word": "hello"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "10", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.7"
  }, 
  "json": null, 
  "origin": "180.42.119.137, 180.42.119.137", 
  "url": "https://httpbin.org/post"
}
'

```

我们传递的参数出现在了form字段中，这表明是模拟了表单提交的方式，以POST方式提交数据。

**timeout参数**

设置超时时间，单位为秒，如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间，它支持HTTP，HTTPS，FTPS。

```python
import urllib.request

responce = urllib.request.urlopen('http://httpbin.org/get',timeout=0.1)
print(responce.read())
```

服务器没有响应，抛出了URLError异常，这个异常属于urllib.error模块，错误原因是超时。因此可以通过一个网页如果长时间未响应，就跳过抓取。利用try except语句来实现，相关代码如下：

```python
import socket
import urllib.request
import urllib.error

try:
    responce = urllib.request.urlopen('http://httpbin.org/get',timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```

判断是超时异常，打印输出了TIME OUT。

**其他参数**

**context**：必须是ssl.SSLContext类型，用来指定SSL设置。

**cafile**和**capath**：分别指定CA证书和它的路径，这个在请求HTTPS链接时比较有用。

**cadefault**：已经弃用，默认为False。

#### 2.Request

如果请求中需要加入Headers等信息，需要更加强大的Request来构建。

实例如下：

```python
import urllib.request

request = urllib.request.Request('http://python.org')
responce = urllib.request.urlopen(request)
print(responce.read().decode('utf-8') )
```

这次urlopen的参数变为Request类型的对象。通过构造这个数据结构，一方面我们可以把请求独立成一个对象，另一方面也可以更加丰富和灵活的配置参数。Request参数说明：

```python
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverfiable=False, method=None)
```

**url**：用于请求URL，这是必传参数，其他都是可选参数。

**data**：如果要传，必须传bytes（字节流）类型。如果是字典，可以先用urllib.parse模块里的urlencode()编码。

**headers**：是一个地点，也就是请求头，可以在构造请求时直接构造，也可以通过调用请求实例的add_header()方法添加。

添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器，默认的User-Agent是Python-urllib，我们可以通过修改它来伪装浏览器，例如伪装成火狐浏览器，可以设置为：

```
Mozilla/5.0 (X11; U; Linux 1686) Gecko/20071127 Firefox/2.0.0.11
```

**origin_req_host**：请求方的host名称或者IP地址。

**unverifiable**：表示这个请求是否是无法验证的，默认是False，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这个时候unverifiable的值就是True。

**method**：是一个字符串，用来指示请求的方法，比如GET，POST和PUT等等。

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    'Host': 'httpbin.org'
}
Dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(Dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
responce = request.urlopen(req)
print(responce.read().decode('utf-8'))
```

运行结果如下：

```
{
  "args": {},
  "data": "",
  "files": {},
  "form": {
    "name": "Germey"
  },
  "headers": {
    "Accept-Encoding": "identity",
    "Content-Length": "11",
    "Content-Type": "application/x-www-form-urlencoded",
    "Host": "httpbin.org",
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)"
  },
  "json": null,
  "origin": "180.42.119.137, 180.42.119.137",
  "url": "https://httpbin.org/post"
}
```

我们成功设置了data，headers和method

另外，headers也可以用add_header()方式添加：

```python
req = request.Request(url=url, data=data, method='POST')
req.add_header('User-Agent','Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')
```

#### 3.高级用法

Handler登场。可以把它理解为各种处理器，利用它，我们可以几乎做到HTTP请求中的所有事情。urllib.request模块里的BaseHandler类，是其他Handler的父类，提供了最基本的方法例如`default_open()`,`protocol_request()`等。接下来就由Handle子类继承这个BaseHandler类，举例如下：

* **HTTPDefaultErrorHandler**：用于处理HTTP响应错误，错误会抛出`HTTPError`类型的异常。
* **HTTPRedirectHandler**：处理重定向
* **HTTPCookieProcessor**：处理Cookies
* **ProxyHandler**：用于设置代理，默认代理为空
* **HTTPPasswordMgr**：用于管理密码，它维护了用户名和密码的表
* **HTTPBasicAuthHandler**：用于管理认证，如果一个链接打开时需要认证，那么用它解决认证问题

另一个重要的类是`OpenerDirector`，我们可以称为`Opener`。我们之前用过urlopen()这个方法，实际上是urllib为我们提供的Opener。

引入Opener是为了实现更高级的功能。之前使用的Request和urlopen()都是类库封装好的常用的请求方法，可以完成基本的请求。实现更高级的功能，需要深入一层进行配置，使用更底层的实例来完成操作，这里就用到了Opener。

Opener可以使用open()方法，返回类型和urlopen()一样。他和Handler有什么关系，就是用Handler来构建Opener。

下面是几个实例：

**验证**

有些网站打开会弹出提示框，直接提示你输入用户名和密码，验证后才能查看页面。请求这样的页面，可以借助HTTPBasicAuthHandler完成，相关代码如下：

```python
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'
password = 'password'
url = 'http://localhost:5000/'

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

首先实例化HTTPBasicAuthHandler对象，其参数是HTTPPasswordMgrWithDefaultRealm对象。它利用add_password()添加进去用户名和密码，这样就建立了一个处理验证的Handler。

接下来使用这个Handler并使用build_open()方法构建一个Opener，这个Opener在发送请求时，就相当于验证成功了。接下来利用Opener的open()方法打开链接，获取验证后的内容。

**代理**

```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743'
    'https': 'http://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
    responce = opener.open('https://www.baidu.com')
    print(responce.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

我们在本地搭建了一个代理，它运行在9743端口上。这里使用了ProxyHandler，参数是一个字典，键名是协议类型（HTTP或HTTPS或其他）。

**Cookies**

先用实例看看如何将网站的Cookies获取下来：

```python
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
responce = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+'='+item.value)
```

声明一个Cookiejar对象，利用HTTPCookieProcessor来构建一个Handler，最后利用build_opener()构建出Opener，执行Open函数。姐夫哦如下：

```
BAIDUID=E8BB6A54F8124D91E9A1A6380F652F41:FG=1
BIDUPSID=E8BB6A54F8124D91970F96B4C0584456
H_PS_PSSID=1451_21082_18560_29568_29699_29221
PSTM=1573206537
delPer=0
BDSVRTM=0
BD_HOME=0
```

这里输出了每一条Cookie的名称和值，也可以通过文件形式保存。

```python
filename = 'cookies.txt'
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
responce = opener.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```



这时候需要换成MozillaCookieJar，生成文件会用到，是CookieJar的子类，用来处理Cookies和相关文件的事件，比如读取和保存Cookies，可以将Cookies保存成Mozilla型浏览器的Cookies文件。生成的cookies.txt内容如下：

```
# Netscape HTTP Cookie File
# http://curl.haxx.se/rfc/cookie_spec.html
# This is a generated file!  Do not edit.

.baidu.com	TRUE	/	FALSE	1604742871	BAIDUID	5F5E54018451ABCAECB3AD6FDCF9C80B:FG=1
.baidu.com	TRUE	/	FALSE	3720690518	BIDUPSID	5F5E54018451ABCAB87B65D6F4F74BE6
.baidu.com	TRUE	/	FALSE		H_PS_PSSID	1452_21093_29567_29700_29220_26350
.baidu.com	TRUE	/	FALSE	3720690518	PSTM	1573206871
.baidu.com	TRUE	/	FALSE		delPer	0
www.baidu.com	FALSE	/	FALSE		BDSVRTM	0
www.baidu.com	FALSE	/	FALSE		BD_HOME	0
```

另外，LWPCookieJar同样可以读取和保存Cookies，但是保存的格式MozillaCookieJar不一样，它会保存成libwww-perl（LWP）格式的Cookies文件。

生成了Cookies文件如何读取和利用：

```python
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
responce = opener.open('http://www.baidu.com')
print(responce.read().decode('utf-8'))
```

load()方法来读取本地的Cookies，获取了Cookies内容。我们之前生成了LWPCookies保存成文件，并保存成文件。之后直接用同样的方法构建Handler和Opener即可完成操作。

### 3.1.2 处理异常

urllib的error模块定义了由request模块产生的异常。如果出现了问题，request模块便会抛出error模块中定义的异常。

#### 1.URLError

来自error模块，继承自OSError类，是error模块的基类，由request模块生成的异常，都可以通过捕获这个类来处理。它有一个属性**reason**，即返回错误的原因。

```python
from urllib import request, error
try:
    responce = request.urlopen('https://huchengyang.org/index.htm')
except error.URLError as e:
    print(e.reason)
```

打开不存在的页面，会捕获异常并输出。

#### 2.HTTPError

是URLError的子类，专门处理HTTP的请求错误。由以下三个属性：

* **code**：返回HTTP状态码。
* **reason**：返回错误原因。
* **headers**：返回请求头。

```python
from urllib import request, error
try:
    responce = request.urlopen('https://www.baidu.com')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, seq='\n')
```

由于URLError是HTTPError的父类，所以可以选择先捕获子类错误，再去捕获父类错误。isintance可以判断错误的类型，做更详细的异常判断。

### 3.1.3 解析链接

urllib模块还提供了parse模块，它定义了标准的URL标准接口。它支持如下URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。

#### 1.urlparse()

可以实现URL的识别和分段，例如

```python
result = urlprase('http://www.baidu.com/index.html;user?id=5#comment')
```

返回结果是一个ParseResult类型的对象。包含六个部分。

://前面的是**scheme**，代表协议；第一个/前是**netloc**，即域名，后面是**path**，即访问路径；分号后面是**param**，代表参数，问好后面是**query**，一般做GET类型的URL；#后面是**锚点**，指定页面内部的下拉位置：

一般标准链接格式如下：

```
scheme://netloc/path;params?query#fragment
```

一个标准的URL都可以这样拆分开。urlparse()方法还有3个参数：

* **urlstring**：必填项，待解析的URL
* **scheme**：它是默认的协议（http或https）假如这个链接没有协议信息，会将这个作为默认的协议。只有在URL不包含scheme信息才会返回参数，如果本身就带有scheme信息，那么设定的信息不起作用。
* **allow_fragments**：是否忽略fragment。如果被设定为False，fragment部分就会被忽略，它会被解析为path，parameter，query的一部分。

返回结果ParseResult是一个元祖，我们可以使用索引顺序来获取，也可以用属性名获取。

#### 2.urlunparse()

这是上一个方法的对立方法。它接受的参数是一个可迭代对象，当时他的长度必须为6，否则会抛出参数不足或者过多的问题：

```python
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

这里用了列表类型，也可以使用其他类型。结果如下：

```
http://www.baidu.com/index.html;user?a=6#comment
```

#### 3.urlspit()

只返回五个结果，上面的params不在解析，会合并在path中。返回结果是SplitResult，也是一个元祖类型，也可以用属性获取值，也可以用索引获取。

#### 4.urlspit()

传入的是一个对象，列表，元祖等等。长度必须为5.

#### 5.urljoin()

我们可以提供一个base_url作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的scheme，netloc和path三个内容并对新链接缺失的内容进行补充。最后返回结果。

base_url提供了三项内容scheme，netloc和path。如果这三项在新的链接里不存在，就予以补充；如果新的链接，就使用新链接部分。此时base_url中的三部分不起作用。

#### 6.urlencode()

在构造GET请求参数很有用，示例如下：

```python
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': '22'
}
base_url = 'https://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

运行结果如下：

```
https://www.baidu.com?name=germey&age=22
```

参数就成功从字典类型转化为GET请求参数。

#### 7.parse_qs()

反序列化。如果我们有一串GET请求参数，利用parse_qs()方法，就可以转回字典。上面的例子而言，传入参数应该是“name=germey&age=22”，不是整个URL。

#### 8. parse_qsl()

可以把参数转化为元组。

#### 9.quote()

该方法可以把内容转化为URL编码的格式。URL带有中文参数时，可以会有乱码问题，用这个可以把汉字转为URL编码。

```python
from urllib.parse import quote

keyword = '壁纸'
url = 'http://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```

结果如下：

```
http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
```

### 3.1.4 分析Robots协议

利用urllib的robotparser模块，我们可以实现网站Robots协议的分析。

#### 1.Robots协议

爬虫协议，机器人协议，全名为网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不能抓取。通常是一个robots.txt文本文件，一般放在根目录下。

当搜索爬虫访问一个节点的时候，会先检查这个站点根目录下是否存在robots.txt，如果存在，搜索爬虫会根据其中定义的范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。

样例：

```
User-agent: *
Disallow: /
Allow: /public/
```

这实现了对所有爬虫只允许爬取public目录的功能。

User-agent描述了搜索爬虫的名称，*代表对任何爬取爬虫有效。

Disallow定义了不允许抓取的目录，上例设置为/代表不允许抓取所有页面。Allow通常和Disallow一起使用，不会单独使用，用来排除某些限制。

#### 2.爬虫名称

![1573219633813](\src\images\1573219633813.png)

#### 3.robotparser

可以用robotparser模块解析robot.txt。该模块提供了一个类RobotFileParser。可以根据robot.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。

声明如下：

```python
urllib.robotparser.RobotFileParser(url='')
```

也可以不传入，默认为空，最后再使用set_url()方法设置。

* **set_url()**：用来设置robots.txt文件的链接，如果创建对象时就已经传入了，就不需要使用这个方法了。
* **read()**：读取robot.txt文件并进行分析。注意，这个方法执行一个读取和分析操作。如果不调用这个方法，接下来的判断都会成为False。这个操作不返回任何内容，但是执行了读取操作。
* **parse()**：用来解析robots.txt内容，传入的参数是robots.txt某些行内容，他会按照robots.txt的语法分析这些内容。
* **can_fetch()**：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果为True或False。
* **mtime()**：返回上次抓取和分析robots.txt的时间，对于长时间分析和抓取爬虫很有必要，你可能需要定期抓取最新的robot.txt、
* **modified()**：将当前时间设置为上次抓取和分析robots.txr的时间。

## 3.2 使用requests

更为强大的库，需要先安装

### 3.2.1 基本用法

#### 1.准备工作

提前安装requests库。

```
pip install requests
```

#### 2.实例引入

requests对应的方法是**get()**方法。

```python
import requests

r = requests.get('https://www.baidu.com')
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.text)
print(r.cookies)
```

运行结果如下：

```
<class 'requests.models.Response'>
200
<class 'str'>
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8>id=head> <di
......
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
```

返回类型为requests.models.Response，响应体类型是字符串str，Cookies的类型是RequestCookieJar。

#### 3.GET请求

构建一个最简单的GET请求：

```python
import requests

r = requests.get('http://httpbin.org/get')
print(r.text)
```

运行结果如下：

```
{
  "args": {},
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Host": "httpbin.org",
    "User-Agent": "python-requests/2.22.0"
  },
  "origin": "180.42.119.137, 180.42.119.137",
  "url": "https://httpbin.org/get"
}
```

我们成功发起了GET请求，返回结果中包含请求头，URL，IP等信息。对于GET请求，如果要附加信息，可以利用params参数

```python
import requests

data = {
    'name': 'germey',
    'age': '22'
}
r = requests.get('http://httpbin.org/get', params=data)
print(r.text)
```

运行结果如下：

```
{
  "args": {
    "age": "22",
    "name": "germey"
  },
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Host": "httpbin.org",
    "User-Agent": "python-requests/2.22.0"
  },
  "origin": "180.42.119.137, 180.42.119.137",
  "url": "https://httpbin.org/get?name=germey&age=22"
}
```

请求链接被自动构成了https://httpbin.org/get?name=germey&age=22。

网页返回的实际上是str类型。但是它很特殊，是JSON格式。想直接解析返回结果，得到字典格式的话，可以直接调用json()方法。

```python
import requests

r = requests.get('http://httpbin.org/get')
print(type(r.text))
print(r.json())
print(type(r.json()))
```

返回结果为

```
<class 'str'>
{'args': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.22.0'}, 'origin': '180.42.119.137, 180.42.119.137', 'url': 'https://httpbin.org/get'}
<class 'dict'>
```

调用方法后，返回结果是JSON格式的字符串转化为字典。如果返回的不是JSON格式，就会出现解析错误。

**●抓取网页**

示例：获得“知乎”→“发现”页。

```python
import requests
import re

headers = {
    'User-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2734.116 Safari/537.6'    
}
r = requests.get('https://www.zhihu.com/explore', headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

这里加入了headers信息，其中包含User-Agent字段信息，也就是浏览器标志信息。不加这个知乎会禁止抓取。

接下来用正则表达式匹配所有问题内容。

**●抓取二进制数据**

如果想抓取图片，音频，视频该怎么办？我们必须抓取到二进制码。

```python
import requests

r = requests.get('https://github.com/favicon.ico')
print(r.text)
print(r.content)
```

第一个打印出来出现了乱码。第二个打印出来前面有个b，代表这里是bytes类型的数据。由于图片是二进制文件，打印成str会乱码。因此我们保存下来：

```python
import requests

r = requests.get('https://github.com/favicon.ico')
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

这里用了open()方法，第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向二进制写入数据。

**●添加headers**

可以见抓取网页

#### 4.POST请求

```python
import requests

data = {'name': 'germey', 'age': '22'}
r = requests.post('http://httpbin.org/post', data=data)
print(r.text)
```

结果如下：

```
{
  "args": {},
  "data": "",
  "files": {},
  "form": {
    "age": "22",
    "name": "germey"
  },
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Content-Length": "18",
    "Content-Type": "application/x-www-form-urlencoded",
    "Host": "httpbin.org",
    "User-Agent": "python-requests/2.22.0"
  },
  "json": null,
  "origin": "180.42.119.137, 180.42.119.137",
  "url": "https://httpbin.org/post"
}
```

#### 5.响应

GET获得的内容还有很多别的信息：

* **status_code**：状态码
* **headers**：响应头，属性结果为CaseInsentiveDict
* **cookies**：Cookies，返回类型为RequestCookiesJar。
* **url**：URL
* **history**：请求历史

request内置一个状态码查询对象request.codes，可以比较返回码和内置返回码。例如成功是requests.codes.ok，得到成功状态码为200.404状态为requests.codes.not_found。

### 3.2.2高级用法

#### 1.文件上传

```python
file = {'file': open('favicon.ico','rb')}
r.requests.post('http://httpbin.org/post', files=files)
```

file字段会有显示。

#### 2.Cookies

获取Cookies过程：

```python
import requests

r = requests.get('http://www.baidu.com')
print(r.cookies)
for key, value in r.cookies.item():
    print(key + value)
```

是RequestCookiesJar类型，用item转换为列表。我们也可以直接用cookies保持登录状态。

登录知乎，直接复制下来Header里的Cookies值。然后加入Headers。

当然可以通过cookies参数设置：

```python
cookies = '...'
jar = requests.cookies.RequestCookieJar()
for cookie in cookies.split(';'):
    key, value = cookies.split('=', 1)
    jar.set(key, value)
r = request.get(url, cookies = jar, headers = headers)
```

新建对象，将复制下来的cookies利用split分割，截止利用set方法设置好每个Cookies的key和value，最后调用get方法即可。

#### 3.会话维持

第一次利用post()方法登陆了某个网站，第二次想获取成功登录后的自己的信息，你有用get()获得。这实际上是打开了两个浏览器。虽然可以用Cookies保持登录，但是太繁琐，这个时候就可以利用Session对象，可以方便的维护一个会话，不用考虑cookies问题

```python
import requests

s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

返回结果如下：

```
{
  "cookies": {
    "number": "123456789"
  }
}
```

在同一个会话里面。不用担心Cookies问题。常用来用于模拟登录成功成功之后的下一步操作。

#### 4.SSL证书验证

当发送HTTP请求的时候，它会检查SSL证书，可以使用verify参数控制是否检查此证书，如果不加，默认是True，自动验证。但是12306没有官方CA验证，会出现验证错误，此时使用get就会显示SSLError错误。如何避免这个错误就是设置为False即可。

当我们设置为False后，会报一个警报，建议我们给它指定证书。我们可以通过设置忽略警告的方式来屏蔽这个警告。

```python
from requests.packages import urllib3

urllib3.disable_warnings()
```

或者：

```python
import logging

logging.captureWarnings(True)
```

当然我们也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或者包含两个文件路径的元祖：

```python
import requests

responce = request.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))
```

我们需要有crt文件和key文件，本地私有证书key必须是解密状态，加密状态不支持。

##### 5.代理设置

需要使用proxies参数。

```python
import requests

proxies = {
    'http': 'http://10.10.1.10:3128'
    'https': 'http://10.10.1.10:1080'
}
request.get('https://www.taobao.com', proxies=proxies)
```

#### 6.超时设置

可以用到timeout参数，单位为秒。但是实际上，请求分为两个阶段连接（connect）和读取（read）。timeout只设定一个数，那就是这两个阶段的总和。如果要分别指定，可以传入一个元祖（长度为3），不加参数或参数为None就是永久等待。

#### 7.身份认证

碰到需要输入用户名和密码的使用，可使用request自带的身份认证功能。

```python
import requests
from requests.auth import HTTPBasicAuth

r = request.get('http://localhost:5000', auth=HTTPBasicAuth('username', 'password'))
print(r.status_code)                                                          
```

如果参数都传一个HTTPBasicAuth类，就显得繁琐。request提供了更简单的写法，直接传一个元祖，它会默认使用HTTPBasicAuth这个类来验证。上面可以简写如下：

```
import requests

r = request.get('http://localhost:5000', auth=('username', 'password'))
print(r.status_code)  
```

#### 8.Prepared Request

urllib时，可以将请求表示为数据结构，其中各个参数都可以通过一个Request对象表示，这在requests中同样可以做到，这个数据结构就叫做Prepared Request。

```python
from requests import Request, Session

s = Session
req = Request('POST', url, data, headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
```

首先构造一个Request对象，这个时候需要调用Session的prepare_request()方法转换为Prepared Request对象，然后调用send()方法发送即可。

进行队列调度非常方便，可以构造Request队列。

## 3.3 正则表达式

#### 1.实例引入

正则表达式测试工具：http://tool.oschina.net/regex/.

![1573278605361](\src\images\1573278605361.png)

Python的re库提供了整个正则表达式的实现

#### 2.match()

传入要匹配的字符串和正则表达式，就可以检测这个正则表达式是否匹配字符串。如果匹配，返回匹配结果，反之返回None。

传出结果为SRE_Match对象，证明成功匹配。有两个内容。一个是group(),返回匹配到的内容，另一个是span()返回匹配的范围。

**●匹配目标**

我们如果想从字符串获得一部分内容，可以使用括号（）括起来，便会称为一个group，之后可以直接用group(i)代表是第i个括号分割的分组，不写就是全部输出。

**●通用匹配**

.*（点星）代表万能匹配。点可以匹配任意字符（换行符除外），\*代表匹配前面的字符无数次。

**●贪婪与非贪婪**

示例字符串如下：

```
Hello 1234567 Demo
```

我们的正则表达式如下：

```
^He.*(\d+).*Demo$
```

group(1)输出结果为7，这是因为贪婪获取使得.*尽可能多的获得字符。因为\d+说明至少有一个字符，因此只匹配到一个字符。如果希望所有数字都匹配到就改为非贪婪匹配。写法是\.\*？（点星问号）尽量使用非贪婪匹配。

需要注意，如果在结尾匹配，使用非贪婪匹配可能没有结果，应该使用贪婪匹配。

**●修饰符**

.\*不能匹配换行符，为了不出错我们可以加入修饰符，作为match的参数。例如可以匹配换行符的修饰符为re.S。

还有一些如下：

![1573279784722](\src\images\1573279784722.png)

**●转义匹配**

如果目标字符串含有已经作为匹配符的字符，前面加上\让它可以匹配。

#### 3.search()

match()是从开头匹配，如果开头不匹配，那么会直接匹配失败。必须考虑开头内容，不方便。

search()会扫描整个字符串，返回第一个成功结果，如果搜索完了都没找到，那么就返回None。为了匹配方便，尽可能使用search()方法。

#### 4.findall()

会返回所有内容，需要进行遍历依次获取每组内容。依次用索引获得每个group。

#### 5.sub()

有三个参数：要匹配的内容，要替换的内容，原内容。

#### 6.compile()

可以将字符串编译成正则表达式对象，以便在之后复用。还可以传入修饰符例如re.S等等。相当对正则表达式做了一层封装。